\chapter{Аналитический раздел}
В рамках данного раздела представлено описание предметной области задачи по извлечению ключевых слов. 
Приведена систематизация методов извлечения КС и текстов.
Произведен отбор и сравнение методов, удовлетворяющих выставленным ограничениям.
Отобран метод на базе которого будуте производиться модификация
Определено направление модификации метода.

\section{Задача автоматического извлечения ключевых слов из текста на естественном языке}

Первые попытки теоретического решения проблемы выделения ключевых ("опорных" или "обобщающих") слов была предпринята в работе А.Н. Соколова "Внутренняя речь и мышление" \cite{6}.
Основы современного понимания ключевых слов, можно сформулировать следующим образом \cite{7}:
\begin{enumerate}
	\item ключевые слова отображают тему текста;
	\item их упорядоченность в наборе ключевых слов может трактоваться как эксплицитно невыраженная тема текста;
	\item набор ключевых слов рассматривается как один из минимальных вариантов "текста";
	\item такого типа "текст" характеризуется "ядерной" цельностью и минимальной связностью
\end{enumerate}

Ключевые слова - это одно или многокомпонентные лексические группы, отражающие содержание документа \cite{3}

Извлечение ключевых слов (Keyword extraction) - это задача по автоматическому определению набора терминов, которые наилучшим образом описывают объект документа.
При изучении терминов, представляющих наиболее релевантную информацию, содержащуюся в документе, используется различная терминология: ключевые фразы, ключевые сегменты, ключевые термины, или просто ключевые слова.
Все выше перечисленные синонимы имеют одну и туже функцию - охарактеризовать обсуждаемую тему в документе \cite{4}.
Извлечение маленького подмножества элементов, включающих от одного до нескольких терминов из одного документа, является важной проблемой в "Информационном поиске" (Information Retrieval, IR), "Интеллектуальном анализе текста" (Text mining, TM) и в "Обработке естественного языка" (Natural Language Processing, NLP).

Ключевые слова нашли широкое применение в запросах к системам информационного поиска, по сколько их легко определить, пересмотреть, запомнить и поделиться.
По сравнению с математическими сигнатурами, они независимы от любого корпуса и могут применяться в нескольких корпусах и системах ИП \cite{5}.
Так же ключевые слова используются для улучшения функциональности информационно-поисковых систем.
Другими словами они могут быть использованы для создания автоматического индекса для коллекции документов или, в качестве альтернативы, могут использоваться для представления документов в задачах категоризации или классификации \cite{1}.

Общая схема извлечения ключевых слов из текста практически одинакова для всех используемых методов и состоит из следующих шагов:
\begin{enumerate}
	\item предварительная обработка текста:
	\item \begin{enumerate}
		\item исключение элементов маркировки;
		\item приведение слова к словарной форме;
		\item удаление стоп слов, не несущих смысловой нагрузки (предлоги, союзы, частицы, местоимения, междометия и т.д.)
	\end{enumerate}
	\item отбор кандидатов в ключевые слова;
	\item фильтрация кандидатов в ключевые слова (анализ значимых признаков для каждого кандидата)
\end{enumerate}

\section{Систематизация методов}
Доступные публикации описывают классификация методов автоматического извлечения КС разной степени полноты и детализации. 
В самом простом случае исследователи выделяют статистические методы и методы, основанные на машином обучении.
Схожая классификация приводится в работе отечественных авторов, которые рассматривают статистические и гибридные модели КС, на основе которых рассматриваются конкретные методы. 
Более развернутая классификация подразумевает выделение четырех стратегий:
\begin{enumerate}
	\item не требующих обучения; 
	\item простых статистических методов;
	\item лингвистических методов;
	\item метод, основанных на машинном обучении, и их комбинаций.
\end{enumerate}

В последних из доступных отечественных обзоров предметной области извлечения ключевых слов проводится классификация на основе типа системы распознавания которая подразумевает выделение \cite{7}:
\begin{enumerate}
	\item лингвистических;
	\item статистических;
	\item гибридных лингвостатистических метов
\end{enumerate}

Однако и эта, и прочие классификации не отражают весь спектр и специфику существующих решений.

Так как любой алгоритм извлечения ключевых слов по сути реализует одну или несколько систем распознавания образов, разбивающих входное множество слов на два класса:
\begin{enumerate}
	\item ключевые;
	\item прочие.
\end{enumerate}

то предлагается использовать не иерархическую, а фасетную классификацию соответствующих методов и выбрать следующую совокупность признаков:
\begin{enumerate}
	\item наличие элемента обучения и подходы к его реализации;
	\item тип математического аппарата системы распознавания, обусловленного формой предстваления признаков ключевых слов;
	\item тип используемых для реализации метода лингвистических ресурсов
\end{enumerate}

По наличию обучения выделяют:
\begin{enumerate}
	\item необучаемые;
	\item обучаемые;
	\item самообучаемые; 
\end{enumerate}
Более простые необучаемые методы подразумевают контекстно независимое выделение КС из отдельного текста на основе априорно составленных моделей и правил. Они подходят для гомогенных по функциональному стилю корпусов текстов, увеличивающихся со временем в объемах, на пример научных работ или нормативных актов.
Обучаемые методы предполагают использование разнообразных лингвистических ресурсов для настройки критериев принятия решений при распознавании ключевых слов.
Здесь большое значение имеет корректное выделение КС в выборке, используемой для обучения.
Среди методов с обучением можно выделить подкласс самообучаемых, если обучение ведется без учителя, или с подкрепление на основе пассивной адаптации.\cite{20}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{src/img/ke_types}
	\caption[]{Классификация методов извлечения ключевых слов}
	\label{fig:ketypes}
\end{figure}

По второму признаку классификации прежде всего стоит выделить статистические и структурные методы извлечения КС.
Статистические методы учитывают относительные частоты встречаемости морфологических, лексических, синтаксических единиц и их комбинации.
Это делает создаваемые на их основе алгоритмы довольно простыми, но недостаточно точными, так как признак частотности ключевых слов не является превалирующим.
Одним из классических методов в данном классе является расчет для каждого слова меры, отражающей его важность в тексте, рассматриваемого как элемент коллекции документов.

В основе структурных методов лежит представление о тексте как о системе семантически и грамматически взаимосвязанных элементов слов которые в свою очередь характеризуются набором лингвистических признаков.
Поэтому многие исследователи называют этот класс методов лингвистическим.
Здесь в первом приближении могут быть выделены два подкласса: графовые и синтаксически шаблонные методы.

Графовые или граф-ориентированные методы представляют текс в виде множества слов-вершин или вершин-словосочетаний и ребер отношений между ними.
Эти отношения могут выражать для каждой пары слов такие факты как последовательное появления в тексте, наличие слова в окне заданного размера и семантическую близость.
Для вершин полученного графа вычисляются меры центральности и по пороговому критерию отбираются ключевые слова.
Различия между данными методами состоят в особеностях учета значимости каждой вершины и вычисления отношений между ними.
В основе синтаксических шаблонных методов лежит представление о регулярных синтаксических конструкциях, содержащих на определенных позициях ключевые слова.
В чистом виде такие методы слабо применимы к рассматриваемой задаче, могут использоваться в сочетании с другими.

Нейросетевые методы к задаче извлечения КС стали применяться сравнительно недавно. 
Они основаны на свойстве искуственных нейронных сетей к обобщению и выделению скрытых зависимостей между входными и выходными данными.
Однако для формирования наборов данных для обучения и функционирования нейронных сетей требуется выделение структурных и статистических признаков, поэтому на практике методы выделения КС являются гибридными, т.е. сочетающими в себе элементы основных расмотренных класов.

Наконец, алгоритмы извлечения КС, реализующие означенные методы, могут не использовать какие либо лингвистические ресурсы или использовать своего рода словари, онтологии, тезаурусы, а так же корпуса текстов с разметкой \cite{20}.

В данной работе не будут рассматриваться методы, требующие обучения или наличия корпуса текстов для обучения.

\section{TF-IDF}
TF-IDF представляет собой классическую числовую метрику, которая показывает релевантность ключевого слова в рамках выбранного документа.
$TF$ (Term Frequency) частота термина равняется количеству повторений кандидата в  общем количестве слов в документе \eqref{eq:tf}.
$IDF$ (Inevrsed Document Frequency) инвертированная частота документа, используется для балансировки излишне повторяющихся слов \eqref{eq:idf}.
Вычисляется как натуральный логарифм от частного - результата деления количества документов, в которых найден термин-кандидат, на общее количество документов.
\begin{equation}
	\label{eq:tf}
	TF = \frac{N_t}{N_{all}}
\end{equation}
где $N_t$ - количество термина в документе; $N_{all}$ - колчество всех слов в документе

\begin{equation}
	\label{eq:idf}
	IDF = \log(N_d / D)
\end{equation}
где $N_d$ - количество документов в которых было найден кандидат; D - общее количество документов.

\section{Rake}
Rapid Authomatic Keyword Extraction (Rake) - это графовый метод использующий лингвистический подход "совместного появления" (сo-occurence) для извлечения ключевых слов из едининичного документа.
Не требует обучения, не зависит от предметой области и языка документа.

Rake базируется на том что ключевые слова чаще всего представляют собой несколько слов и редко содержат пунктуацию и стоп слова такие как предлоги, речевые обороты и другие не значимые слова с минимальным лексическим значением. \cite{15}

\section{Yake}

Алгоритм Yake относится категории комбинированных методов, который использует подходы как статистических методов, так и графовых. 
Состоит из 4 главных шагов:

\begin{enumerate}
	\item предварительная обработка текста и определение термина-кандидата;
	\item извлечение свойств;
	\item вычисление счета термина;
	\item вычисление дублей и ранжирование.
\end{enumerate}
На первом шаге обрабатывается документ в машино-читаемый формат для определения потенциальных терминов-кандидатов.
Это важный и ключевой шаг, от которого зависит качество определения кандидатов, что на прямую влияет на эффективность самого алгоритма.
Следующей этап принимает на вход массив индивидуальных терминов и репрезентует их в виде набора статистических свойств.
На третьем этапе эти свойства эвристически объединяются в единую оценку которая отражает важность значения термина.
На финальном этапе сравниваются вероятно похожие термины на основе измерений схожести, полученных от специальных алгоритмов.

Алгоритм на вход получает текст и необходимые параметры: размера окна w (используется для вычисления одного из статистических свойств), порог повторения $\theta$ и язык текста, последний строчный параметр используется для получения списка стоп слов.
Полученный текст метод начинает разделять на предложения.
Каждое предложение в результате предобработки превращается в некоторое количество терминов, что завершает первый этап.
На следующем шаге для каждого термина высчитываются его статистические свойства и подсчитывается оценка.
На завершающем шаге происходит отсеивание кандидатов на ключевое слово, где свойство схожести больше $\theta$.
Дальше список ключевых слов и оценки сортируются по релевантности и возвращаются данным методом, что завершает работу алгоритма.
Ниже приведено более детальное описание каждого шага.

\subsection{Предварительная обработка текста и определение списка кандидатов}
Предварительная обработка текста - это первый этап, после которого идет репрезентация текста и  его анализ.
На данном этапе идет очищение текста и трансформация его в машино-читаемый формат: выделены важные части и удалены шумовые слова.
Классическая предобработка включает в себя: очистку текста, разбиение на предложения, аннотирование текста, токенизацию и определение стоп слов.
Так же могут использоваться техники обработки текстов на естественном языке (natural language processing, NLP): 
\begin{enumerate}
	\item пометка частей речи (part-of-speech tagging, PoS);
	\item распознавание именованных объектов (named-entity recognition, NER);
	\item нормализация;
	\item лингвистический разбор или стемминг;
\end{enumerate}
однако, для этого требуются специальные инструменты, каждый из которых работает с тем языком для которого он был реализован.
В рамках этапа данного алгоритма они не используются.

Поступивший текст алгоритм начинает делить на предложения. 
Это достигается путем применения сегментера текстов основанного на правилах, который разделяет индоевропейские языки на высказывания следую предопределенному шаблону.
На пример: "Python is awesome! But C++ is also very good" будет разделено на 2 выражения ("Python is awesome!", "But is awesome") в то время как "Mr. Smith" будет одиночным выражением.
Затем каждое высказывание делится на группы (по найденой пунктуации) и затем проходит процесс токенизации.
Это очень важный этап который не только позволяет выделить слова, но и отсеять шум в виде пунктуации, адресов электронных почт, и ссылок, которые мешают при работе с огромными текстами.
После этого каждый токен приводится к нижнему регистру и помечается специальной меткой разделителем.

Результат предобработки - это список предложений, разделенных на группы, сформированные из помеченных терминов.
В следующем разделе приводится описание статистических свойств, использующихся в данном методе.

\subsection{Вычисление свойств}

После предварительной обработки и получения кандидатов применяется статистический анализ, который уделяет внимание структуре, частоте терминов и их сочетаемости.
В начале создается пустая структура, которая будет хранить в себе одиночные термины, найденные в тексте, и дополнительную информацию, такую как результаты статистических метрик и оценку (вычисляется позже).
Затем перебираем список выражений и чанки.
Кажый чанк разбивается на ранее размеченные токены и для них вычисляются:
\begin{enumerate}
	\item частота термина (TF, term frequency);
	\item индекс выражений, где встречается данный токен (offsets\_sentences);
	\item частота акронима термина (TF\_a, term frequency of acronym);
	\item частота слова в старшем регистре (TF\_U, term frequency of uppercase);
\end{enumerate}
в дополнении к ним так же вычисляется матрица сочетаемости для сохранения связи между термином и его предшественником или подтермины, которые найдены в окне размером w.
После того, как статистика каждого термина вычислена, можно проводить процесс извлечения признаков.
Дли одиночного термина извлекаются следующие признаки: TCase, TPos, TFNorm, Trel и Tsent, детальное описание каждого признака идет ниже.

\subsection{Регистр ($T_{Case}$)}

Аспект термина связанный с регистром является важной характеристикой при рассмотрении извлечения ключевых слов.
Основная идея состоит в том, что термины в верхнем регистре, как правило, более релевантны, чем слова в нижнем регистре. 
В представленом подходе уделяется особое внимание любому термину, начинающемуся с заглавной буквы (исключая начало предложений), тем более аббревиатуры где все буквы слова заглавные.
Однако вместо того, чтобы считать это двойным весом, мы будем рассматривать только максимальное вхождение в пределах двух из них. 
Уравнение \eqref{eq:tcase} отражает внешний вид термина-кандидата:
\begin{equation}
	\label{eq:tcase}
	T_{Case} = \frac{max(TF(U(t))), TF(A(t))}{ln(TF(t))}
\end{equation}
где $TF(U(t))$ количество термина-кандидата t начинающегося с заглавной буквы, $TF(A(t))$ сколько раз кандидат t был отмечен как акроним и $TF(t)$ - это частота t.
Таким образом чем чаще кандидат пишется с заглавной буквы, тем более важным он считаетсся.
Это означает, что термин-кандидат, встречающийся с заглавной буквой 10 из 10 случаев будет иметь большее значение, чем коллега, встретившийся 5 раз из 5 случаев.

\subsection{Позиция термина ($T_{position}$)}
Другим индикатором важности кандидата является позиция.
Причина в том, что релевантные слова, как правило, появляются в самом начале документа, тогда как слова, встречающиеся в середине или в конце документа, имеют тенденцию быть менее важными.
Особенно это очивидно как для новостных статей, так и для научных текстов, двух видов публикаций, которые склонны концентрировать большое количество важных ключевых слов в верхней части текста.
Например, во ведении или в аннотации.
Предположение авторов состоит в том, что термины, встречающиеся в первых предложениях текста должны быть более высоко оценены, чем термины, которые появляются позже. 
Таким образом, вместо того, чтобы рассматривать равномерное распределение терминов,
их модель присваивает более высокие баллы терминам, встречающимся в первых предложениях. 
Вес рассчитывается, используя следующее уравнение \eqref{eq:tposition}:
\begin{equation}
	\label{eq:tposition}
	T_{position} = ln(ln(3 + Median(Sen_t)))
\end{equation}
где $Sen_t$ множество позиций в выражениях, где кандидат t появляется и медиана от $Sen_t$.
Зная что медианая функция $Median(Sen_t)$ может возвращать значение 0 (когда термн-кандидат появляется только в первом предложении), к уравнению добавляются константа $C > e$, в случае данной реализации 3 (как первое число после $e$), что бы гарантировать что $T_{position} > 0$.
Так же для сглаживания разницы между терминами с большой средней разницей используется двойной логарифм.

Так для термина-кандидата $t_1$ который появляется в 2, 35, 70, 74, 4000 выражениях в документе будет иметь $T_{position} = ln(ln(3 + Median(2, 35, 70, 74, 4000))) = 1.45$ в то время как $t_2$, появляющийся в 1, 4 и 7 выражении получит $T_position = ln(ln(3 + Median(1, 4, 7)) = 0.66$.
Результатом является возрастающая функция, значения которой имеют тенденцию плавно возрастать по мере того, как термины-кандидаты располагаются ближе к концу документа, а это значит, что чем больше кандидатов появляются в начале документа, тем ниже его значение $T_{position}$ и наоборот терминам (менее актуальные), расположенным ближе к концу документа, будет присвоено более высокое значение $T_{position}$.

\subsection{Нормализация частоты термина ($TF_{norm}$)}
Эта функция указывает частоту  термина-кандидата $t$ в документе на основе работе Луна \cite{18}, которая утверждает что "частота появления слова в тексте обеспечивает полезное измерение значения слова".
Данное высказывание отображает убеждение, что чем выше частота кандидата, тем выше его важность.
Тем не менее, это не означает что важность пропорциональна тому сколько раз встречается термин.
Таким образом для предотвращения смещения в сторону высоких частот в длинных документах значение $TF$ кандидата $t$ делится на среднее значение частот (MeanTF) плюс 1, умноженное на стандартное отклонение \eqref{eq:tfnorm}.

\begin{equation}
	\label{eq:tfnorm}
	TF_{norm} = \frac{TF(t)}{MeanTF + 1 * \sigma}
\end{equation} 
Цель заключается в том, что бы оценить все термины-кандидаты, частоты которых выше среднего, сбалансированного определенной степенью дисперсии, заданной стандартным отклонением.
Чтобы вычислить это авторы метода решили учитывать только MeanTF и стандартое отклонение не стоп-слов, что гарантирует, что на вычисления этих двух компонентов не влияют высокие частоты, регистрируемые по стоп-словам.

\subsection{Связь термина с контекстом}
Хотя стоп-слова представляют собой бесспорно полезный источник знаний о том, что явно не относится к делу, только полагаясь на статическом списке,информации может оказаться недостаточно для отбрасывания нерелевантных слов.
В данном разделе описывается статистическая функция, которая нацелена на определение дисперсии (D) термина-кандидата t относительно его конкретного контекста, опирающаяся на работу Machado \cite{19}, утверждающая что чем выше число различных терминов, которые встречаются вместе с терминов-кандидатом t c обеих сторон, тем менее значимым будет термин t

\begin{figure}[!h]
	\centering
	\includegraphics[width=1\linewidth]{src/img/coocurrences_example}
	\caption{Совместное появление при w = 2}
	\label{fig:coocurrencesexample}
\end{figure}

\begin{equation}
	DL[DR] = \frac{|A_t, w|}{\sum_{k \in A_{t, w}CoOccur_{t, k}}} 
\end{equation}
где $|A_t, w|$ представляет собой количество различных терминов, где термин представляет собой анализируемое содержимое, аббревиатуру, или слово в верхнем регистре, которое появляется слева(справа) от термина-кандидата t в заданном окне размера w \eqref{fig:coocurrencesexample} в отношении k терминов с которыми он встречается.

Затем DL и DR умножаются на частоту термина-кандидата, деленную на максимальную частоту термина среди всех кандидатов, которые встречаются в документе.
Окончательное уравнение имеет вид \eqref{eq:trel}:
\begin{equation}
	\label{eq:trel}
	T_{Rel} = 1 + (DL + DR) * \frac{TF_t}{MaxTF}
\end{equation}
где крайняя левая часть уравнения измеряет значимость термина-кандидата по отношению к его левой части, а
самая правая часть измеряет свою значимость по отношению к правой стороне. С практической точки зрения, чем менее релевантен кандидат t, тем выше будет оценка этой функции.
Таким образом, стоп-слова и, аналогичным образом, недискриминационные термины, как правило, получают более высокий балл. На рис. \eqref{fig:coocurrencesexample} показаны различные термины, которые встречаются вместе с термином-кандидатом «relations» в пределах w, равного 2, где «relations» — неважный термин (не часть истины, т. е. не золотое ключевое слово) в контексте нашего текущего примера.
В правой части рисунка показаны отрывки из текста, в которых встречается термин «отношения», а в левой — графическое представление. 
Число на ребрах — это частота, с которой термин-кандидат «relations» встречается вместе в соответствующем овале. 
Серым цветом обозначены стоп-слова.

\subsection{Другое предложение термина ($T_{sentence}$)}

Это свойство подсчитывает как часто термин-кандидат встречается в других предложениях.
Оно отображает, что кандидат, встречающийся во многих других предложениях, можеть быть более важным.
Оценка подчитывается по следующей формуле:

\begin{equation}
	T_{sentence} = \frac{SF(t)}{Sentences}
\end{equation}
где $SF(t)$ - количество предложений в которых появляется t.
$Sentences$ - это максимальное количесвто предолжений в тексте.
Результат лежит в диапозоне от $[0, 1]$


\subsection{Подсчет оценки термина}
После того как все оценки свойств получены, можно подсчитать финальный результат оценки термина.
Все полученные ранее оценки подставляются в выражение S(t) \eqref{eq:termin_score}.

\begin{equation}
	\label{eq:termin_score}
	S(t) = \frac{T_{Rel} * T_{Postion}}{T_{Case} + \frac{TF_{Norm}}{T_{Rel}} + \frac{T_{sentences}}{T_{Rel}}}
\end{equation}
Стоит обратить внимание на то что $TF_{Norm}$ и $TF_{sentensec}$ делятся на $T_{Rel}$.
Это делается для того чтобы присвоить большое значение терминам, которые появляются часто во многих предложениях, до тех пор пока они релевантны.
Поскольку, некоторые кандидаты могут встречаться много раз в многих предложениях и при этом быть бесполезными и должны быть "оштрафованы".
Таким образом достижение высокой оценки по $TF_{Norm}$ и $TF_{sentensec}$ является индикацией важности термина при низком значении $T_{Rel}$.
Так же важным свойством является позиция появления термина, она добавляется в выражение путем умножения $T_{Rel} * T_{Postion}$

\section{Сравнение методов}
\begin{enumerate}
	\item TF-IDF
	\item Плюсы \begin{enumerate}
		\item простой.
	\end{enumerate}
	\item Минусы \begin{enumerate}
		\item требует наличия большого корпуса текстов;
		\item не работает со словосочетаниями
		\item оценка происходит только по частотной характеристеке термина
	\end{enumerate}
	\item Rake
	\item Плюсы \begin{enumerate}
		\item простой
		\item не требует наличия корпуса текстов;
		\item учитывает взаимосвязи слов в тексте.
	\end{enumerate}
	\item Минусы \begin{enumerate}
		\item оценка происходит только по частотной характеристике термина
		\item не выдает словосочетания
	\end{enumerate}
	\item Yake
	\item Плюсы \begin{enumerate}
		\item учитывает взаимосвязи слов;
		\item учитывает позицию кандидата;
		\item учитывает его написание (абревиатуры, регистры);
		\item учитывается связь с контекстом
	\end{enumerate}
	\item Минусы \begin{enumerate}
		\item Не выдает словосочетания
	\end{enumerate}
\end{enumerate}

Учитывая все ранее выставленные требования к алгоритмам было принято решение использовать метод Yake.
До этого данный метод не использовался для вычисления ключевых слов из текста на русском языке и для этого необходимо провести следующие модификации:
\begin{enumerate}
	\item добавление сборника стоп-слов;
	\item добавление работы с N-gram;
\end{enumerate}

\section{Модификация}
Шумовые слова (или стоп-слова) — термин из теории поиска информации по ключевым словам.
Это такие слова, знаки, символы, которые самостоятельно не несут никакой смысловой нагрузки, но
которые, тем не менее, совершенно необходимы для нормального восприятия текста, его целостности.
К ним относятся предлоги, суффиксы, причастия, междометия, цифры, частицы и т. п.
Так как данные слова имеют самые большие частоты встречи в документах, то для более корректного результата небходимо данные слова исключать из текста, по этому есть необходимость в добавлении в метод списка стоп-слов для русского языка.

Как было описано ранее, результатом работы метода Yake являются одиночные ключевые слова с оценкой.
Современные технологии непрерывно развиваются.
Это значительно влияет на научно-технический функциональный стиль, в частности, на его терминосистемы.
Появляются новые термины и усложняются старые \cite{22}.
В результате растет количество многокомпонентных терминов.
По этому есть необходимость в модификации выбранного метода для возможности извлечения многокомпонентных ключевых слов.
Данную модификацию проведем с помощью добавления n-грамм.
Пусть задан некоторый конечный алфавит $V = {w_i}$, где $w_i$ - символ. 
Языком $L(V)$ называют множество цепочек конечной длины из символов $w_i$.
Высказыванием называют цепочку из языка.
N-граммой на алфавите V называют произвольную цепочку длинной N, например последовательность из N букв русского языка, одного слова, одной фразы, одного текста \cite{21}.

Конечный результат будет высчитываться по формуле \eqref{eq:ngram}: 
\begin{equation}
	\label{eq:ngram}
	S(kw) = \frac{\prod_{t \in kw}S(t)}{KF(kw) * (1 + \sum_{t \in kw}S(t))}
\end{equation}
где $S(kw)$ - финальный результат оценки, чем ниже тем реливантнее, $S(t)$ - оценка одиночного термина,
$KF(kw)$ - частота ключевого слова(выражения)

\section{Заключение}
В рамках данного раздела представлено описание предметной области задачи по извлечению ключевых слов. 
Приведена систематизация методов извлечения КС и текстов.
Произведен отбор и сравнение методов удовлетворяющих выставленным ограничениям.
Отобран метод на базе которого будет проводиться модификация
Определены направления модификации выбранного метода Yake.


